{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20aee05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import MultivariateNormal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283d1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha,\n",
    "            fc1_dims=256, fc2_dims=256, chkpt_dir= \"CHKPT\"):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "#         self.checkpoint_file = chkpt_dir\n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, n_actions)\n",
    "               \n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "#         dist = self.actor(state)\n",
    "#         dist = Categorical(dist)\n",
    "        mean = self.actor(state)\n",
    "#         dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        return mean\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
    "            chkpt_dir=\"CHKPT\"):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,'critic_torch_ppo')\n",
    "\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df17a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "        # Initialize the covariance matrix used to query the actor for actions\n",
    "        self.cov_var = T.full(size = (n_actions,), fill_value = 0.5).to('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        #Create the covariance matrix\n",
    "        self.cov_mat = T.diag(self.cov_var).to('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        mean = self.actor(state)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample().to(self.actor.device)\n",
    "        action = T.squeeze(action)\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = action.cpu()\n",
    "        action = action.detach().numpy()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "    \n",
    "                mean = self.actor(states)\n",
    "                dist = MultivariateNormal(mean, self.cov_mat)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space  2\n",
      "... saving models ...\n",
      "episode 0 score -281.5 avg score -281.5 time_steps 96 learning_steps 1\n",
      "... saving models ...\n",
      "episode 1 score -93.1 avg score -187.3 time_steps 171 learning_steps 3\n",
      "... saving models ...\n",
      "episode 2 score -65.5 avg score -146.7 time_steps 259 learning_steps 5\n",
      "episode 3 score -216.0 avg score -164.0 time_steps 350 learning_steps 7\n",
      "episode 4 score -734.7 avg score -278.2 time_steps 517 learning_steps 10\n",
      "episode 5 score -447.8 avg score -306.4 time_steps 623 learning_steps 12\n",
      "episode 6 score -176.7 avg score -287.9 time_steps 718 learning_steps 14\n",
      "episode 7 score -445.4 avg score -307.6 time_steps 846 learning_steps 16\n",
      "episode 8 score -494.6 avg score -328.4 time_steps 961 learning_steps 19\n",
      "episode 9 score -79.4 avg score -303.5 time_steps 1050 learning_steps 21\n",
      "episode 10 score -194.7 avg score -293.6 time_steps 1181 learning_steps 23\n",
      "episode 11 score -163.0 avg score -282.7 time_steps 1313 learning_steps 26\n",
      "episode 12 score -234.7 avg score -279.0 time_steps 1398 learning_steps 27\n",
      "episode 13 score -273.4 avg score -278.6 time_steps 1505 learning_steps 30\n",
      "episode 14 score -672.3 avg score -304.9 time_steps 1626 learning_steps 32\n",
      "episode 15 score -91.0 avg score -291.5 time_steps 1715 learning_steps 34\n",
      "episode 16 score -92.6 avg score -279.8 time_steps 1857 learning_steps 37\n",
      "episode 17 score -79.7 avg score -268.7 time_steps 1962 learning_steps 39\n",
      "episode 18 score -81.8 avg score -258.8 time_steps 2080 learning_steps 41\n",
      "episode 19 score -17.2 avg score -246.8 time_steps 2183 learning_steps 43\n",
      "episode 20 score -174.8 avg score -243.3 time_steps 2264 learning_steps 45\n",
      "episode 21 score -29.1 avg score -233.6 time_steps 2429 learning_steps 48\n",
      "episode 22 score -134.5 avg score -229.3 time_steps 2641 learning_steps 52\n",
      "episode 23 score -263.1 avg score -230.7 time_steps 2815 learning_steps 56\n",
      "episode 24 score -283.6 avg score -232.8 time_steps 2921 learning_steps 58\n",
      "episode 25 score -280.7 avg score -234.7 time_steps 3077 learning_steps 61\n",
      "episode 26 score -328.1 avg score -238.1 time_steps 3172 learning_steps 63\n",
      "episode 27 score -143.2 avg score -234.7 time_steps 3279 learning_steps 65\n",
      "episode 28 score -111.5 avg score -230.5 time_steps 3374 learning_steps 67\n",
      "episode 29 score -156.5 avg score -228.0 time_steps 3463 learning_steps 69\n",
      "episode 30 score -300.2 avg score -230.3 time_steps 3553 learning_steps 71\n",
      "episode 31 score -61.9 avg score -225.1 time_steps 3649 learning_steps 72\n",
      "episode 32 score -74.7 avg score -220.5 time_steps 3728 learning_steps 74\n",
      "episode 33 score 14.3 avg score -213.6 time_steps 3848 learning_steps 76\n",
      "episode 34 score -182.9 avg score -212.7 time_steps 3940 learning_steps 78\n",
      "episode 35 score -300.3 avg score -215.2 time_steps 4054 learning_steps 81\n",
      "episode 36 score -191.0 avg score -214.5 time_steps 4155 learning_steps 83\n",
      "episode 37 score -90.2 avg score -211.2 time_steps 4300 learning_steps 86\n",
      "episode 38 score -76.9 avg score -207.8 time_steps 4432 learning_steps 88\n",
      "episode 39 score -232.0 avg score -208.4 time_steps 4547 learning_steps 90\n",
      "episode 40 score -117.9 avg score -206.2 time_steps 4626 learning_steps 92\n",
      "episode 41 score -281.9 avg score -208.0 time_steps 4814 learning_steps 96\n",
      "episode 42 score -97.5 avg score -205.4 time_steps 4964 learning_steps 99\n",
      "episode 43 score 104.9 avg score -198.4 time_steps 5964 learning_steps 119\n",
      "episode 44 score -59.5 avg score -195.3 time_steps 6111 learning_steps 122\n",
      "episode 45 score -165.6 avg score -194.6 time_steps 6210 learning_steps 124\n",
      "episode 46 score -353.9 avg score -198.0 time_steps 6381 learning_steps 127\n",
      "episode 47 score -389.9 avg score -202.0 time_steps 6539 learning_steps 130\n",
      "episode 48 score -453.4 avg score -207.2 time_steps 6795 learning_steps 135\n",
      "episode 49 score -326.7 avg score -209.6 time_steps 6979 learning_steps 139\n",
      "episode 50 score -342.8 avg score -212.2 time_steps 7146 learning_steps 142\n",
      "episode 51 score -20.5 avg score -208.5 time_steps 7260 learning_steps 145\n",
      "episode 52 score -374.0 avg score -211.6 time_steps 7394 learning_steps 147\n",
      "episode 53 score -326.6 avg score -213.7 time_steps 7603 learning_steps 152\n",
      "episode 54 score -139.9 avg score -212.4 time_steps 7781 learning_steps 155\n",
      "episode 55 score -306.7 avg score -214.1 time_steps 7933 learning_steps 158\n",
      "episode 56 score -498.6 avg score -219.1 time_steps 8073 learning_steps 161\n",
      "episode 57 score -505.2 avg score -224.0 time_steps 8398 learning_steps 167\n",
      "episode 58 score -317.9 avg score -225.6 time_steps 8561 learning_steps 171\n",
      "episode 59 score -467.5 avg score -229.6 time_steps 8847 learning_steps 176\n",
      "episode 60 score -196.5 avg score -229.1 time_steps 8944 learning_steps 178\n",
      "episode 61 score -328.2 avg score -230.7 time_steps 9119 learning_steps 182\n",
      "episode 62 score -271.8 avg score -231.3 time_steps 9337 learning_steps 186\n",
      "episode 63 score -365.8 avg score -233.4 time_steps 9429 learning_steps 188\n",
      "episode 64 score -627.9 avg score -239.5 time_steps 9691 learning_steps 193\n",
      "episode 65 score -292.6 avg score -240.3 time_steps 9833 learning_steps 196\n",
      "episode 66 score -377.9 avg score -242.4 time_steps 10004 learning_steps 200\n",
      "episode 67 score -435.4 avg score -245.2 time_steps 10203 learning_steps 204\n",
      "episode 68 score -408.7 avg score -247.6 time_steps 10318 learning_steps 206\n",
      "episode 69 score -40.8 avg score -244.6 time_steps 10420 learning_steps 208\n",
      "episode 70 score -59.9 avg score -242.0 time_steps 10562 learning_steps 211\n",
      "episode 71 score -93.7 avg score -240.0 time_steps 10695 learning_steps 213\n",
      "episode 72 score -38.2 avg score -237.2 time_steps 10828 learning_steps 216\n",
      "episode 73 score -112.2 avg score -235.5 time_steps 10951 learning_steps 219\n",
      "episode 74 score -89.8 avg score -233.6 time_steps 11046 learning_steps 220\n",
      "episode 75 score -24.2 avg score -230.8 time_steps 11129 learning_steps 222\n",
      "episode 76 score -82.9 avg score -228.9 time_steps 11233 learning_steps 224\n",
      "episode 77 score -115.5 avg score -227.4 time_steps 11320 learning_steps 226\n",
      "episode 78 score -47.3 avg score -225.1 time_steps 11424 learning_steps 228\n",
      "episode 79 score 1.9 avg score -222.3 time_steps 11552 learning_steps 231\n",
      "episode 80 score -128.1 avg score -221.1 time_steps 11688 learning_steps 233\n",
      "episode 81 score -43.0 avg score -219.0 time_steps 11784 learning_steps 235\n",
      "episode 82 score -64.1 avg score -217.1 time_steps 11908 learning_steps 238\n",
      "episode 83 score -55.5 avg score -215.2 time_steps 12071 learning_steps 241\n",
      "episode 84 score -16.7 avg score -212.8 time_steps 12227 learning_steps 244\n",
      "episode 85 score -55.6 avg score -211.0 time_steps 12322 learning_steps 246\n",
      "episode 86 score -320.9 avg score -212.3 time_steps 12588 learning_steps 251\n",
      "episode 87 score -369.9 avg score -214.1 time_steps 12821 learning_steps 256\n",
      "episode 88 score -496.2 avg score -217.2 time_steps 12994 learning_steps 259\n",
      "episode 89 score -328.9 avg score -218.5 time_steps 13273 learning_steps 265\n",
      "episode 90 score -231.8 avg score -218.6 time_steps 13407 learning_steps 268\n",
      "episode 91 score -395.3 avg score -220.6 time_steps 13552 learning_steps 271\n",
      "episode 92 score -236.1 avg score -220.7 time_steps 13726 learning_steps 274\n",
      "episode 93 score -384.2 avg score -222.5 time_steps 13959 learning_steps 279\n",
      "episode 94 score -389.0 avg score -224.2 time_steps 14302 learning_steps 286\n",
      "episode 95 score -35.7 avg score -222.2 time_steps 14534 learning_steps 290\n",
      "episode 96 score -409.9 avg score -224.2 time_steps 14771 learning_steps 295\n",
      "episode 97 score -222.6 avg score -224.2 time_steps 15021 learning_steps 300\n",
      "episode 98 score -63.4 avg score -222.5 time_steps 15270 learning_steps 305\n",
      "episode 99 score -23.0 avg score -220.5 time_steps 15480 learning_steps 309\n",
      "episode 100 score 19.3 avg score -217.5 time_steps 15756 learning_steps 315\n",
      "episode 101 score -39.6 avg score -217.0 time_steps 15959 learning_steps 319\n",
      "episode 102 score -199.9 avg score -218.3 time_steps 16127 learning_steps 322\n",
      "episode 103 score -101.6 avg score -217.2 time_steps 16380 learning_steps 327\n",
      "episode 104 score 24.5 avg score -209.6 time_steps 17380 learning_steps 347\n",
      "episode 105 score 123.5 avg score -203.9 time_steps 18121 learning_steps 362\n",
      "episode 106 score -135.7 avg score -203.5 time_steps 18342 learning_steps 366\n",
      "episode 107 score -114.0 avg score -200.2 time_steps 18876 learning_steps 377\n"
     ]
    }
   ],
   "source": [
    "    def plot_learning_curve(x, scores, figure_file):\n",
    "        running_avg = np.zeros(len(scores))\n",
    "        for i in range(len(running_avg)):\n",
    "            running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "        plt.plot(x, running_avg)\n",
    "        plt.title('Running average of previous 100 scores')\n",
    "        plt.savefig(figure_file)\n",
    "    \n",
    "    env = gym.make('LunarLanderContinuous-v2')\n",
    "    print(\"action space \", env.action_space.shape[0])\n",
    "    N = 50\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    agent = Agent(n_actions=env.action_space.shape[0], batch_size=batch_size, \n",
    "                    alpha=alpha, n_epochs=n_epochs, \n",
    "                    input_dims=env.observation_space.shape[0])\n",
    "    n_games = 500\n",
    "\n",
    "    figure_file = 'plots/cartpole.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "#         env.render()\n",
    "        done = False\n",
    "        score = 0\n",
    "        t = 0\n",
    "        reward_ep = []\n",
    "        while (not done) and (t<1000):\n",
    "            t+=1\n",
    "            \n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            #print(\"this is the action => \",action,\" \",action.shape)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "        reward_ep.append(score)\n",
    "        avg = sum(reward_ep)/len(reward_ep)\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score ,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "#     env.close()\n",
    "\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history, figure_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66852850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d91269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d43982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
